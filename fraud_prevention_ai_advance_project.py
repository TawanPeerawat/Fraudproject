# -*- coding: utf-8 -*-
"""Fraud Prevention AI Advance Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z8NQf1QaH5_UBB1EJm48CzP8gepRRocB
"""

# üì¶ STEP 1: IMPORT LIBRARIES
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import xgboost as xgb
import seaborn as sns
import matplotlib.pyplot as plt

# üì• STEP 2: LOAD DATA (‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ)
cols_to_use = [
    "Fraud_flag", "Name_Code", "Age", "Loan_amount", "Occupation", "Gender",
    "Experience", "Income", "Customer_province", "Current_address_province", "Work_address_province",
    "Product", "Vehicle_brand", "Release_year", "Guarantor_name_1", "Guarantor_name_2",
    "Channel", "Owner_Case", "Region_Flag", "Phone_Number"
]
df = pd.read_excel("/content/Fraud Data for model.xlsx", usecols=cols_to_use)

# üßπ STEP 3: CLEAN + CONVERT TO BINARY
df["Fraud_flag"] = df["Fraud_flag"].apply(lambda x: 1 if x in [1, 2] else 0)

# ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢: ‡πÅ‡∏ó‡∏ô missing ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
df["Name_Code"] = df["Name_Code"].fillna("Unknown")
df["Phone_Number"] = df["Phone_Number"].fillna("Unknown")
df["Guarantor_name_1"] = df["Guarantor_name_1"].fillna("None")
df["Guarantor_name_2"] = df["Guarantor_name_2"].fillna("None")

# üß† STEP 4: FEATURE ENGINEERING (4 ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà)
df["num_guarantor"] = df[['Guarantor_name_1', 'Guarantor_name_2']].apply(
    lambda x: sum([1 for i in x if i != "None"]), axis=1)

phone_count = df["Phone_Number"].value_counts()
df["phone_count"] = df["Phone_Number"].map(phone_count)

guarantor_counts = pd.concat([
    df[["Guarantor_name_1"]],
    df[["Guarantor_name_2"]].rename(columns={"Guarantor_name_2": "Guarantor_name_1"})
])
guarantor_counts = guarantor_counts["Guarantor_name_1"].value_counts()
df["guarantee_others"] = df["Name_Code"].map(guarantor_counts).fillna(0)

guarantor_links = pd.concat([
    df[["Name_Code", "Guarantor_name_1"]].rename(columns={'Name_Code': 'A', 'Guarantor_name_1': 'B'}),
    df[["Guarantor_name_1", "Name_Code"]].rename(columns={'Guarantor_name_1': 'A', 'Name_Code': 'B'}),
    df[["Name_Code", "Guarantor_name_2"]].rename(columns={'Name_Code': 'A', 'Guarantor_name_2': 'B'}),
    df[["Guarantor_name_2", "Name_Code"]].rename(columns={'Guarantor_name_2': 'A', 'Name_Code': 'B'}),
])
guarantor_links = guarantor_links[guarantor_links['A'] != guarantor_links['B']]
guarantee_network_count = guarantor_links.groupby('A').nunique()['B']
df['guarantee_network_size'] = df['Name_Code'].map(guarantee_network_count).fillna(0)

# üéØ STEP 5: SPLIT FEATURES AND TARGET
features = [
    "Age", "Loan_amount", "Occupation", "Gender", "Experience", "Income",
    "Customer_province", "Current_address_province", "Work_address_province",
    "Product", "Vehicle_brand", "Release_year", "Channel", "Owner_Case", "Region_Flag",
    "Phone_Number", "Name_Code",
    "num_guarantor", "phone_count", "guarantee_others", "guarantee_network_size"
]
X = df[features].copy()
y = df["Fraud_flag"]

# üîÑ STEP 6: ENCODE + HANDLE MISSING
for col in X.select_dtypes(include="object").columns:
    X[col] = LabelEncoder().fit_transform(X[col].astype(str))

imputer = SimpleImputer(strategy="most_frequent")
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# üîÄ STEP 7: SPLIT TRAIN/TEST
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ‚öôÔ∏è STEP 8: TRAIN XGBOOST
fraud_weight_ratio = (y_train == 0).sum() / (y_train == 1).sum()
model = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric="logloss",
    scale_pos_weight=fraud_weight_ratio
)
model.fit(X_train, y_train)

# üìà STEP 9: EVALUATE
y_pred = model.predict(X_test)
print("‚úÖ Classification Report:\n")
print(classification_report(y_test, y_pred, digits=4))

# üìä STEP 10: CONFUSION MATRIX
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print(df["Fraud_flag"].value_counts())

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 300],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

grid = GridSearchCV(
    estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    param_grid=param_grid,
    scoring='f1',
    cv=3,
    n_jobs=-1
)
grid.fit(X_train, y_train)

print("‚úÖ Best Parameters from GridSearchCV:")
print(grid.best_params_)

best_model = grid.best_estimator_

y_pred = best_model.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
print("üìä Classification Report:\n")
print(classification_report(y_test, y_pred, digits=4))

# ‡πÅ‡∏™‡∏î‡∏á confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix (Best XGBoost)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á feature importance
feature_importance = pd.Series(best_model.feature_importances_, index=X_train.columns)
feature_importance = feature_importance.sort_values(ascending=False)

# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
feature_importance.plot(kind='bar', figsize=(12,6), title="Feature Importance")
plt.tight_layout()
plt.show()

!pip install neo4j pandas

!pip install networkx pyvis

import pandas as pd
import networkx as nx

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Name_Code, Guarantor_name_1, Guarantor_name_2)
df = pd.read_excel("/content/Fraud Data for model.xlsx", usecols=[
    "Name_Code", "Guarantor_name_1", "Guarantor_name_2", "Fraud_flag"
])
df = df.fillna("None")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á graph ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏ó‡∏¥‡∏® (undirected ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤‡∏ß‡∏á)
G = nx.Graph()

# ‡πÄ‡∏û‡∏¥‡πà‡∏° node ‡∏û‡∏£‡πâ‡∏≠‡∏° fraud_flag
for _, row in df.iterrows():
    borrower = row['Name_Code']
    G.add_node(borrower, fraud=row.get('Fraud_flag', 0))

    for g in [row['Guarantor_name_1'], row['Guarantor_name_2']]:
        if g != "None" and g != borrower:
            G.add_node(g)
            G.add_edge(g, borrower)

central = nx.degree_centrality(G)
top_guarantors = sorted(central.items(), key=lambda x: x[1], reverse=True)[:10]
print("Top connected nodes:", top_guarantors)

fraud_rings = [c for c in nx.connected_components(G) if len(c) >= 4]
print("‡∏û‡∏ö fraud ring:", len(fraud_rings), "‡∏Å‡∏•‡∏∏‡πà‡∏°")